digraph FoghornDataFlowVertical {
  rankdir=TB;
  node [shape=box, fontsize=10];

  // =========================
  // 1. Clients (top)
  // =========================
  subgraph cluster_clients {
    label="Clients";
    style=dashed;
    color=gray;

    Clients [label="DNS Clients\n(stub resolvers, apps,\nother recursors)"];
  }

  // =========================
  // 2. Ingress: downstream protocols
  // =========================
  subgraph cluster_ingress {
    label="Ingress: Downstream Protocols";
    style=filled;
    color=lightgrey;

    L_UDP_Inline [label="UDP Listener\nThreadingUDPServer\n+ DNSUDPHandler.handle\n(legacy inline UDP)"];

    L_UDP_Generic [label="UDP Listener\nThreadingUDPServer\n+ _UDPHandler.handle\n(src/foghorn/udp_server.py)\nresolver=resolve_query_bytes"];

    L_TCP_Async [label="TCP Listener\nasyncio.start_server\nserve_tcp()\n(src/foghorn/tcp_server.py)"];

    L_TCP_Threaded [label="TCP Listener\nThreadingTCPServer\nserve_tcp_threaded()\n(src/foghorn/tcp_server.py)"];

    L_DOT [label="DoT Listener\nasyncio TLS server\nserve_dot()\n(src/foghorn/dot_server.py)"];

    L_DOH_FastAPI [label="DoH Listener\nFastAPI /dns-query\ncreate_doh_app()\n(src/foghorn/doh_api.py)"];

    L_DOH_Threaded [label="DoH Listener\nHTTPServer + _ThreadedDoHRequestHandler\n(src/foghorn/doh_api.py)"];
  }

  // Edges: clients → listeners
  Clients -> L_UDP_Inline   [label="UDP 53"];
  Clients -> L_UDP_Generic  [label="UDP 53"];
  Clients -> L_TCP_Async    [label="TCP 53"];
  Clients -> L_TCP_Threaded [label="TCP 53"];
  Clients -> L_DOT          [label="TLS 853"];
  Clients -> L_DOH_FastAPI  [label="HTTP(S)\n/dns-query"];
  Clients -> L_DOH_Threaded [label="HTTP(S)\n/dns-query"];

  // =========================
  // 3. Core resolver entry
  // =========================
  subgraph cluster_entry {
    label="Core Resolver Entry\n(DNSUDPHandler.handle / resolve_query_bytes)";
    style=filled;
    color="#e0f7ff";

    Entry_UDP [label="DNSUDPHandler.handle\n(UDP inline path)\n• data,sock from BaseRequestHandler\n• client_address"];

    Entry_Generic [label="resolve_query_bytes(data, client_ip)\n(used by UDP generic,\nTCP, DoT, DoH)\n• returns wire response"];
  }

  // Listener → core entry
  L_UDP_Inline   -> Entry_UDP     [label="(data,sock)"];
  L_UDP_Generic  -> Entry_Generic [label="resolver(data, client_ip)"];
  L_TCP_Async    -> Entry_Generic [label="resolver(body, client_ip)"];
  L_TCP_Threaded -> Entry_Generic [label="resolver(body, client_ip)"];
  L_DOT          -> Entry_Generic [label="resolver(body, client_ip)"];
  L_DOH_FastAPI  -> Entry_Generic [label="resolver(body, client_ip)"];
  L_DOH_Threaded -> Entry_Generic [label="resolver(body, client_ip)"];

  // =========================
  // 4. Parse, context, and initial stats
  // =========================
  subgraph cluster_parse {
    label="Parse & Context Setup";
    style=filled;
    color="#e0f7ff";

    Parse_UDP [label="UDP path:\nDNSUDPHandler._parse_query(data)\n• DNSRecord.parse\n• q = req.questions[0]\n• qname=str(q.qname).rstrip('.')\n• qtype=q.qtype\n• cache_key=(qname.lower(),qtype)\n• PluginContext(client_ip)"];

    Parse_Generic [label="resolve_query_bytes:\nDNSRecord.parse(data)\n• qname,qtype\n• cache_key=(qname.lower(),qtype)\n• PluginContext(client_ip)"];

    Record_Query [label="If StatsCollector set:\nrecord_query(client_ip,\nqname, qtype_name)"];
  }

  Entry_UDP    -> Parse_UDP;
  Entry_Generic -> Parse_Generic;

  // For simplicity, treat both parse paths as feeding into Record_Query
  Parse_UDP    -> Record_Query;
  Parse_Generic -> Record_Query;

  // =========================
  // 5. Pre-plugins (with caches) and pre decisions
  // =========================
  subgraph cluster_pre_plugins {
    label="Pre-Resolve Plugins\n(BasePlugin.pre_resolve)";
    style=filled;
    color="#f0e0ff";

    Pre_BuildCtx [label="PluginContext(client_ip)\n(upstream_candidates=None,\nupstream_override=None)\n(if not already built)"];

    Pre_Loop_UDP [label="UDP path:\nfor p in sorted(DNSUDPHandler.plugins,\n                key=pre_priority):\n  decision = p.pre_resolve(qname,qtype,data,ctx)"];

    Pre_Loop_Generic [label="Generic path:\nfor p in sorted(DNSUDPHandler.plugins,\n                key=pre_priority):\n  decision = p.pre_resolve(qname,qtype,data,ctx)"];

    Pre_Deny [label="If decision.action=='deny':\n• UDP: synth NXDOMAIN via\n  request.reply(), rcode=NXDOMAIN\n  → _set_response_id\n  → sendto(sock)\n• Generic: NXDOMAIN reply,\n  _set_response_id → return wire\nStats:\n• record_response_rcode('NXDOMAIN')\n• record_query_result(status='deny_pre',\n  source='pre_plugin')"];

    Pre_Override [label="If decision.action=='override'\n and decision.response is not None:\n• Use decision.response as reply\n• _set_response_id(reply, req.id)\nStats:\n• parse reply\n• record_response_rcode(rcode)\n• record_query_result(status='override_pre',\n  source='pre_plugin_override')\n• Send/return wire"];
  }

  Record_Query -> Pre_BuildCtx;
  Pre_BuildCtx -> Pre_Loop_UDP;
  Pre_BuildCtx -> Pre_Loop_Generic;

  // Pre decisions → early exits or proceed
  Pre_Loop_UDP    -> Pre_Deny     [label="decision.deny"];
  Pre_Loop_Generic -> Pre_Deny    [label="decision.deny"];
  Pre_Loop_UDP    -> Pre_Override [label="decision.override"];
  Pre_Loop_Generic -> Pre_Override [label="decision.override"];

  // =========================
  // 6. DNS response cache: lookup
  // =========================
  subgraph cluster_dns_cache {
    label="DNS Response Cache\n(DNSUDPHandler.cache : TTLCache)";
    style=filled;
    color="#fff4d9";

    Cache_Lookup [label="TTLCache.get(cache_key)\nkey=(qname.lower(), qtype)\n• expired → evict & return None"];

    Cache_Hit_Stats [label="On hit:\n• DNSRecord.parse(cached)\n• record_cache_hit(qname)\n• record_response_rcode(rcode)\n• record_query_result(\n    status='cache_hit',\n    source='cache',\n    answers=parsed.rr)"];

    Cache_Hit_Send [label="On hit:\n• _set_response_id(cached, req.id)\n• UDP: sock.sendto(...)\n• Generic: return wire"];

    Cache_Miss_Stats [label="On miss:\nrecord_cache_miss(qname)"];
  }

  // From pre-plugins (no deny/override) into cache lookup
  Pre_Loop_UDP    -> Cache_Lookup [label="no decision or allow"];
  Pre_Loop_Generic -> Cache_Lookup [label="no decision or allow"];

  Cache_Lookup -> Cache_Hit_Stats [label="if cached!=None"];
  Cache_Hit_Stats -> Cache_Hit_Send;

  Cache_Lookup -> Cache_Miss_Stats [label="if cached is None"];

  // =========================
  // 7. Upstream selection
  // =========================
  subgraph cluster_upstream_select {
    label="Upstream Selection";
    style=filled;
    color="#e3ffe3";

    Upstream_Choose_UDP [label="UDP path:\nupstreams =\n  ctx.upstream_candidates\n  or DNSUDPHandler.upstream_addrs"];

    Upstream_Choose_Generic [label="Generic path:\nupstreams =\n  ctx.upstream_candidates\n  or DNSUDPHandler.upstream_addrs"];

    No_Upstreams [label="If not upstreams:\n• Build SERVFAIL reply\n  (request.reply(), rcode=SERVFAIL)\n• _set_response_id\n• UDP: sock.sendto(...)\n• Generic: return wire\nStats:\n• record_response_rcode('SERVFAIL')\n• record_query_result(status='no_upstreams',\n  source='server')"];
  }

  Cache_Miss_Stats -> Upstream_Choose_UDP;
  Cache_Miss_Stats -> Upstream_Choose_Generic;

  Upstream_Choose_UDP    -> No_Upstreams [label="if empty"];
  Upstream_Choose_Generic -> No_Upstreams [label="if empty"];

  // =========================
  // 8. EDNS / DNSSEC DO bit
  // =========================
  subgraph cluster_edns {
    label="EDNS / DNSSEC DO Adjustment";
    style=filled;
    color="#e0f7ff";

    EDNS_UDP [label="DNSUDPHandler._ensure_edns(req)\n• find/replace/add OPT RR\n• rclass=edns_udp_payload\n• DO bit if dnssec_mode=='passthrough'"];

    EDNS_Generic [label="resolve_query_bytes:\ninline EDNS helper\n(similar logic)\nfor dnssec_mode in\n['ignore','passthrough']"];
  }

  Upstream_Choose_UDP    -> EDNS_UDP    [label="if upstreams"];
  Upstream_Choose_Generic -> EDNS_Generic [label="if upstreams"];

  // =========================
  // 9. Upstream forwarding with failover
  // =========================
  subgraph cluster_upstream_forward {
    label="Upstream Forwarding & Failover";
    style=filled;
    color="#ffe0e0";

    SQWF_UDP [label="UDP path helper:\nDNSUDPHandler._forward_with_failover_helper\n→ send_query_with_failover(req, upstreams,\n                            timeout_ms, qname, qtype)"];

    SQWF_Generic [label="Generic path:\nsend_query_with_failover(req, upstreams,\n                          timeout_ms, qname, qtype)"];

    SQWF_Core [label="send_query_with_failover:\nfor each upstream in list:\n  • Determine transport\n  • Log attempt\n  • Call transport\n  • Parse reply\n  • Handle SERVFAIL / truncation\n  • On success: return\nOn exhaustion: (None,None,'all_failed')"];

    U_UDP [label="UDP transport\nudp_query(host,port, query.pack(), timeout_ms)\n(or query.send() fallback)"];

    U_TCP [label="TCP transport\nget_tcp_pool(host,port)\nTCPConnectionPool.set_limits(...)\nTCPConnectionPool.send(query.pack(),\n                       connect_timeout_ms,\n                       read_timeout_ms)"];

    U_DOT [label="DoT transport\nget_dot_pool(host,port, server_name,\n            verify, ca_file)\nDotConnectionPool.set_limits(...)\nDotConnectionPool.send(query.pack(),\n                       connect_timeout_ms,\n                       read_timeout_ms)"];

    U_DOH [label="DoH transport\n.doh.doh_query(url, query.pack(),\n               method, headers,\n               timeout_ms,\n               verify, ca_file)\n→ (body, resp_headers)"];

    Upstream_Parse [label="parse response_wire:\nparsed = DNSRecord.parse(response_wire)\n• if rcode==SERVFAIL → try next\n• if transport=='udp' and TC=1:\n    tcp_query() fallback,\n    return (wire, {transport:'tcp'}, 'ok')\n• else: return (wire, upstream, 'ok')"];

    Upstream_AllFailed [label="If all upstreams raise / fail:\nreturn (None, None, 'all_failed')"];

    Upstream_Stats [label="If StatsCollector:\n• record_upstream_result(upstream_id,\n                         outcome='success'|'timeout'|...)\n• later: record_upstream_rcode(upstream_id, rcode)"];
  }

  EDNS_UDP    -> SQWF_UDP;
  EDNS_Generic -> SQWF_Generic;

  SQWF_UDP    -> SQWF_Core;
  SQWF_Generic -> SQWF_Core;

  SQWF_Core -> U_UDP [label="transport='udp'"];
  SQWF_Core -> U_TCP [label="transport='tcp'"];
  SQWF_Core -> U_DOT [label="transport='dot'"];
  SQWF_Core -> U_DOH [label="transport='doh'"];

  U_UDP -> Upstream_Parse;
  U_TCP -> Upstream_Parse;
  U_DOT -> Upstream_Parse;
  U_DOH -> Upstream_Parse;

  Upstream_Parse -> Upstream_Stats;
  Upstream_Parse -> Upstream_AllFailed [label="on repeated\nSERVFAIL/parse err"];

  // =========================
  // 10. Upstream failure → SERVFAIL
  // =========================
  subgraph cluster_upstream_fail {
    label="Upstream Failure Handling";
    style=filled;
    color="#ffe0e0";

    Upstream_Fail_Reply [label="If reply is None:\n• Build SERVFAIL via\n  _make_servfail_response(req)\n  or generic path\n• UDP: send once directly,\n  and once via\n  _cache_and_send_response\n  (which will not cache SERVFAIL)\n• Generic: SERVFAIL wire via\n  _set_response_id\nStats:\n• record_response_rcode('SERVFAIL')\n• record_upstream_rcode(upstream_id,'SERVFAIL')\n• record_query_result(status=reason,\n  source='upstream',\n  error='all_upstreams_failed')"];
  }

  Upstream_AllFailed -> Upstream_Fail_Reply;

  // =========================
  // 11. Post-plugins (with caches) and overrides
  // =========================
  subgraph cluster_post_plugins {
    label="Post-Resolve Plugins\n(BasePlugin.post_resolve)";
    style=filled;
    color="#e0f0ff";

    Post_ClearFlag [label="Before loop:\nclear ctx._post_override\n(if present)"];

    Post_Loop_UDP [label="UDP path:\nfor p in sorted(DNSUDPHandler.plugins,\n                key=post_priority):\n  decision = p.post_resolve(qname,qtype,reply,ctx)"];

    Post_Loop_Generic [label="Generic path:\nfor p in sorted(DNSUDPHandler.plugins,\n                key=post_priority):\n  decision = p.post_resolve(qname,qtype,reply,ctx2)"];

    Post_Deny [label="If decision.action=='deny':\n• Build NXDOMAIN reply\n  (request.reply(), rcode=NXDOMAIN)\n• break loop"];

    Post_Override [label="If decision.action=='override'\n and decision.response is not None:\n• reply = decision.response\n• UDP: set ctx._post_override=True\n• break loop"];
  }

  Upstream_Parse -> Post_ClearFlag [label="on success"];
  Post_ClearFlag -> Post_Loop_UDP;
  Post_ClearFlag -> Post_Loop_Generic;

  Post_Loop_UDP    -> Post_Deny     [label="decision.deny"];
  Post_Loop_Generic -> Post_Deny    [label="decision.deny"];
  Post_Loop_UDP    -> Post_Override [label="decision.override"];
  Post_Loop_Generic -> Post_Override [label="decision.override"];

  // =========================
  // 12. DNSSEC validation (after post-plugins)
  // =========================
  subgraph cluster_dnssec {
    label="DNSSEC Validation (optional)";
    style=filled;
    color="#f6e0ff";

    DNSSEC_Check [label="If dnssec_mode=='validate':\nparsed = DNSRecord.parse(reply)"];

    DNSSEC_UpstreamAD [label="If dnssec_validation=='upstream_ad':\nvalid = (parsed.header.ad == 1)\nif not valid:\n  rewrite to SERVFAIL"];

    DNSSEC_Local [label="If dnssec_validation=='local':\ncall validate_response_local(\n  qname,qtype,reply,\n  udp_payload_size)\nif not valid:\n  rewrite to SERVFAIL"];

    DNSSEC_Fail [label="On DNSSEC error/exception:\n• rewrite to SERVFAIL\n• r = req.reply(); rcode=SERVFAIL\n• reply = r.pack()"];
  }

  Post_Deny     -> DNSSEC_Check;
  Post_Override -> DNSSEC_Check;
  // Also path where no post-plugin decision:
  Post_Loop_UDP    -> DNSSEC_Check [label="no decision"];
  Post_Loop_Generic -> DNSSEC_Check [label="no decision"];

  DNSSEC_Check -> DNSSEC_UpstreamAD [label="validation='upstream_ad'"];
  DNSSEC_Check -> DNSSEC_Local      [label="validation='local'"];

  DNSSEC_UpstreamAD -> DNSSEC_Fail [label="if not valid"];
  DNSSEC_Local      -> DNSSEC_Fail [label="if not valid"];

  // =========================
  // 13. Cache store & response send
  // =========================
  subgraph cluster_cache_store {
    label="Cache Store & Response Send";
    style=filled;
    color="#fffbe0";

    Compute_TTL [label="compute_effective_ttl(resp, min_cache_ttl):\n• if rcode==NOERROR and answers:\n    ttls = [rr.ttl]\n    effective_ttl = max(min(ttls), min_cache_ttl)\n• else:\n    effective_ttl = max(0, min_cache_ttl)\n• on error: fallback to min_cache_ttl"];

    Cache_Store [label="If rcode!=SERVFAIL\nand effective_ttl>0:\nTTLCache.set(cache_key,\n             effective_ttl,\n             response_wire)"];

    Legacy_Cache [label="_cache_store_if_applicable(...):\n• DNSRecord.parse(response_wire)\n• never cache SERVFAIL\n• only cache NOERROR+answers\n• if min(answer_ttl)<=0: do not cache\n• else use compute_effective_ttl\n  and TTLCache.set(...)"];

    SetID [label="_set_response_id(wire, req.id):\n• normalize to bytes\n• _set_response_id_cached(bwire, req_id)\n(lru_cache(maxsize=1024))"];

    Send_UDP [label="UDP path:\nsock.sendto(response_wire,\n            client_address)"];

    Return_Generic [label="Generic path (TCP/DoT/DoH/UDP-generic):\nreturn wire to caller\n• TCP: framing handled\n  in tcp_server/dot_server\n• DoH: Response(content=wire)"];
  }

  // Paths into cache+send for success replies
  DNSSEC_Check -> Compute_TTL;
  DNSSEC_Fail  -> Compute_TTL;

  Compute_TTL -> Cache_Store;
  Cache_Store -> SetID;

  // Legacy helper used in some SERVFAIL paths
  Upstream_Fail_Reply -> Legacy_Cache;

  Legacy_Cache -> SetID;

  // Final send/return
  SetID -> Send_UDP      [label="DNSUDPHandler.handle"];
  SetID -> Return_Generic [label="resolve_query_bytes\n+ higher-level servers"];

  // =========================
  // 14. DNS response cache object itself
  // =========================
  subgraph cluster_dns_cache_obj {
    label="DNS Cache Object\n(TTLCache impl)";
    style=filled;
    color="#fff4d9";

    TTLCache_Class [label="TTLCache (src/foghorn/cache.py)\n_store: {(qname,str,qtype,int): (expiry,float,wire,bytes)}\nMethods:\n• get(key): check expiry, evict, return\n• set(key, ttl, data): store,\n  opportunistic purge\n• purge_expired()"];
  }

  Cache_Lookup -> TTLCache_Class [style=dashed, dir=both, label="get()"];
  Cache_Store  -> TTLCache_Class [style=dashed, label="set()"];
  Legacy_Cache -> TTLCache_Class [style=dashed, label="set()"];

  // =========================
  // 15. Plugin decision caches (per plugin)
  // =========================
  subgraph cluster_plugin_caches {
    label="Plugin Decision Caches\n(inheritable_ttl_cache)";
    style=filled;
    color="#f5e5ff";

    Plugin_Cache_Pre [label="BasePlugin.pre_resolve\n@inheritable_ttl_cache\nkey=(qname,int(qtype))\n→ per-subclass TTLCache\n(cache_ttl, cache_maxsize)"];

    Plugin_Cache_Post [label="BasePlugin.post_resolve\n@inheritable_ttl_cache\nkey=(qname,int(qtype), response_wire)\n→ per-subclass TTLCache"];
  }

  Pre_Loop_UDP    -> Plugin_Cache_Pre  [style=dashed, label="lookup/store\npre decisions"];
  Pre_Loop_Generic -> Plugin_Cache_Pre [style=dashed, label="lookup/store\npre decisions"];
  Post_Loop_UDP    -> Plugin_Cache_Post [style=dashed, label="lookup/store\npost decisions"];
  Post_Loop_Generic -> Plugin_Cache_Post [style=dashed, label="lookup/store\npost decisions"];

  // =========================
  // 16. Transport connection pools (as caches)
  // =========================
  subgraph cluster_conn_pools {
    label="Transport Connection Pools\n(cached upstream connections)";
    style=filled;
    color="#ffe6e6";

    TCP_Pool [label="TCPConnectionPool\n• key=(host,port)\n• stack of _TCPConn\n• set_limits(max_connections,\n             idle_timeout_s)\n• send(query,\n       connect_timeout_ms,\n       read_timeout_ms)"];

    DOT_Pool [label="DotConnectionPool\n• key=(host,port,server_name,\n       verify,ca_file)\n• stack of _DotConn\n• set_limits(max_connections,\n             idle_timeout_s)\n• send(query,\n       connect_timeout_ms,\n       read_timeout_ms)"];
  }

  U_TCP -> TCP_Pool [style=dashed, dir=both, label="get_tcp_pool()\nreuses _TCPConn"];
  U_DOT -> DOT_Pool [style=dashed, dir=both, label="get_dot_pool()\nreuses _DotConn"];

  // =========================
  // 17. Helper LRU caches & registries
  // =========================
  subgraph cluster_helpers {
    label="Helper Caches & Registries";
    style=filled;
    color="#dddddd";

    Cache_ResponseID [label="_set_response_id_cached\nlru_cache(maxsize=1024)\nkey=(wire_bytes,req_id)\nvalue=wire_with_new_id"];

    Cache_DomainNorm [label="stats._normalize_domain\nlru_cache(maxsize=1024)\nnormalize domain\n(lowercase, strip '.')"];

    Plugin_Registry [label="plugins.registry:\n• _camel_to_snake\n• _default_alias_for\n• _normalize(alias)\nall lru_cache\n• discover_plugins():\n  alias→plugin class map"];

    Stats_TopK [label="Stats TopK structures\n(in-memory counters)\nused for top domains,\nclients, etc."];
  }

  SetID -> Cache_ResponseID [style=dashed, label="calls"];

  // =========================
  // 18. Statistics & observability
  // =========================
  subgraph cluster_stats {
    label="Statistics & Observability";
    style=filled;
    color="#e0ffe8";

    Stats_Query [label="record_query(...)\nqname,qtype,client_ip"];
    Stats_Cache [label="record_cache_hit/miss(...)\ncache events"];
    Stats_Upstream [label="record_upstream_result(...)\nrecord_upstream_rcode(...)"];
    Stats_Rcode [label="record_response_rcode(rcode)"];
    Stats_QueryResult [label="record_query_result(...)\nsource, status, answers,\nerrors, upstream_id"];
    Stats_Latency [label="record_latency(seconds)\nLatencyHistogram\n(count, min, max, avg,\n p50, p90, p99)"];
  }

  // Wires from main path into stats
  Record_Query     -> Stats_Query  [style=dotted];
  Cache_Hit_Stats  -> Stats_Cache  [style=dotted];
  Cache_Miss_Stats -> Stats_Cache  [style=dotted];
  Upstream_Stats   -> Stats_Upstream [style=dotted];

  Upstream_Fail_Reply -> Stats_Rcode   [style=dotted];
  DNSSEC_Fail         -> Stats_Rcode   [style=dotted];
  Cache_Hit_Stats     -> Stats_Rcode   [style=dotted];

  // Final response logging
  SetID -> Stats_Rcode      [style=dotted];
  SetID -> Stats_QueryResult [style=dotted];
  SetID -> Stats_Latency    [style=dotted];

  // =========================
  // 19. Egress back to clients
  // =========================
  subgraph cluster_egress {
    label="Egress: Back to Clients\n(via original listeners)";
    style=filled;
    color=lightgrey;

    Egress_UDP  [label="UDP: sock.sendto(...)"];
    Egress_TCP  [label="TCP: length-prefix\nwrite in tcp_server"];
    Egress_DOT  [label="DoT: TLS length-prefix\nwrite in dot_server"];
    Egress_DOH  [label="DoH: HTTP(S) response\nFastAPI or threaded HTTP"];
  }

  // Map SetID/Return_Generic to specific listener behaviors
  Send_UDP      -> Egress_UDP;
  Return_Generic -> Egress_TCP;
  Return_Generic -> Egress_DOT;
  Return_Generic -> Egress_DOH;

  // Conceptual responses back to clients
  Egress_UDP -> Clients [style=dashed, label="DNS response"];
  Egress_TCP -> Clients [style=dashed, label="DNS response"];
  Egress_DOT -> Clients [style=dashed, label="DNS response"];
  Egress_DOH -> Clients [style=dashed, label="DNS response"];
}
